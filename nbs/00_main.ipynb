{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main\n",
    "\n",
    "> This is how it is originally done. Clean it up with future modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'boostmonodepth_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2c1cb45c6741>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvispy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mboostmonodepth_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrun_boostmonodepth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbilateral_filtering\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse_bilateral_filtering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mMiDaS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrun_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'boostmonodepth_utils'"
     ]
    }
   ],
   "source": [
    "# Potential Reqirements\n",
    "# torch==1.4.0+cu100 torchvision==0.5.0+cu100 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# opencv-python==4.2.0.32\n",
    "# vispy==0.6.4\n",
    "# moviepy==1.0.2\n",
    "# transforms3d==0.3.1\n",
    "# networkx==2.3\n",
    "# sudo apt install sed\n",
    "\n",
    "# pip install torch>=1.4.0 torchvision>=0.5.0 opencv-python==4.2.0.32 vispy==0.6.4 moviepy==1.0.2 transforms3d==0.3.1 networkx==2.3\n",
    "## Others\n",
    "# pip install scikit-image \n",
    "\n",
    "\n",
    "import argparse\n",
    "import copy\n",
    "from functools import partial\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "import cv2\n",
    "import imageio\n",
    "import numpy as np\n",
    "import scipy.misc as misc\n",
    "from skimage.transform import resize\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import vispy\n",
    "\n",
    "from boostmonodepth_utils import run_boostmonodepth\n",
    "from bilateral_filtering import sparse_bilateral_filtering\n",
    "from MiDaS.run import run_depth\n",
    "from MiDaS.monodepth_net import MonoDepthNet\n",
    "import MiDaS.MiDaS_utils as MiDaS_utils\n",
    "from mesh import write_ply, read_ply, output_3d_photo\n",
    "from networks import Inpaint_Color_Net, Inpaint_Depth_Net, Inpaint_Edge_Net\n",
    "from utils import get_MiDaS_samples, read_MiDaS_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vispy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4bac69728758>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mvispy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmisc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vispy'"
     ]
    }
   ],
   "source": [
    "# Original Code https://github.com/vt-vl-lab/3d-photo-inpainting\n",
    "# Original Huggingface Space Code from https://huggingface.co/spaces/Epoching/3D_Photo_Inpainting/tree/main\n",
    "\n",
    "# main.py\n",
    "import numpy as np\n",
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "from functools import partial\n",
    "import vispy\n",
    "import scipy.misc as misc\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import time\n",
    "import sys\n",
    "from mesh import write_ply, read_ply, output_3d_photo\n",
    "from utils import get_MiDaS_samples, read_MiDaS_depth\n",
    "import torch\n",
    "import cv2\n",
    "from skimage.transform import resize\n",
    "import imageio\n",
    "import copy\n",
    "from networks import Inpaint_Color_Net, Inpaint_Depth_Net, Inpaint_Edge_Net\n",
    "from MiDaS.run import run_depth\n",
    "from boostmonodepth_utils import run_boostmonodepth\n",
    "from MiDaS.monodepth_net import MonoDepthNet\n",
    "import MiDaS.MiDaS_utils as MiDaS_utils\n",
    "from bilateral_filtering import sparse_bilateral_filtering\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--config', type=str, default='argument.yml',help='Configure of post processing')\n",
    "args = parser.parse_args()\n",
    "config = yaml.load(open(args.config, 'r'))\n",
    "if config['offscreen_rendering'] is True:\n",
    "    vispy.use(app='egl')\n",
    "os.makedirs(config['mesh_folder'], exist_ok=True)\n",
    "os.makedirs(config['video_folder'], exist_ok=True)\n",
    "os.makedirs(config['depth_folder'], exist_ok=True)\n",
    "sample_list = get_MiDaS_samples(config['src_folder'], config['depth_folder'], config, config['specific'])\n",
    "normal_canvas, all_canvas = None, None\n",
    "\n",
    "if isinstance(config[\"gpu_ids\"], int) and (config[\"gpu_ids\"] >= 0):\n",
    "    device = config[\"gpu_ids\"]\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"running on device {device}\")\n",
    "\n",
    "for idx in tqdm(range(len(sample_list))):\n",
    "    depth = None\n",
    "    sample = sample_list[idx]\n",
    "    print(\"Current Source ==> \", sample['src_pair_name'])\n",
    "    mesh_fi = os.path.join(config['mesh_folder'], sample['src_pair_name'] +'.ply')\n",
    "    image = imageio.imread(sample['ref_img_fi'])\n",
    "\n",
    "    print(f\"Running depth extraction at {time.time()}\")\n",
    "    if config['use_boostmonodepth'] is True:\n",
    "        run_boostmonodepth(sample['ref_img_fi'], config['src_folder'], config['depth_folder'])\n",
    "    elif config['require_midas'] is True:\n",
    "        run_depth([sample['ref_img_fi']], config['src_folder'], config['depth_folder'],\n",
    "                  config['MiDaS_model_ckpt'], MonoDepthNet, MiDaS_utils, target_w=640)\n",
    "\n",
    "    if 'npy' in config['depth_format']:\n",
    "        config['output_h'], config['output_w'] = np.load(sample['depth_fi']).shape[:2]\n",
    "    else:\n",
    "        config['output_h'], config['output_w'] = imageio.imread(sample['depth_fi']).shape[:2]\n",
    "    frac = config['longer_side_len'] / max(config['output_h'], config['output_w'])\n",
    "    config['output_h'], config['output_w'] = int(config['output_h'] * frac), int(config['output_w'] * frac)\n",
    "    config['original_h'], config['original_w'] = config['output_h'], config['output_w']\n",
    "    if image.ndim == 2:\n",
    "        image = image[..., None].repeat(3, -1)\n",
    "    if np.sum(np.abs(image[..., 0] - image[..., 1])) == 0 and np.sum(np.abs(image[..., 1] - image[..., 2])) == 0:\n",
    "        config['gray_image'] = True\n",
    "    else:\n",
    "        config['gray_image'] = False\n",
    "    image = cv2.resize(image, (config['output_w'], config['output_h']), interpolation=cv2.INTER_AREA)\n",
    "    depth = read_MiDaS_depth(sample['depth_fi'], 3.0, config['output_h'], config['output_w'])\n",
    "    mean_loc_depth = depth[depth.shape[0]//2, depth.shape[1]//2]\n",
    "    if not(config['load_ply'] is True and os.path.exists(mesh_fi)):\n",
    "        vis_photos, vis_depths = sparse_bilateral_filtering(depth.copy(), image.copy(), config, num_iter=config['sparse_iter'], spdb=False)\n",
    "        depth = vis_depths[-1]\n",
    "        model = None\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Start Running 3D_Photo ...\")\n",
    "        print(f\"Loading edge model at {time.time()}\")\n",
    "        depth_edge_model = Inpaint_Edge_Net(init_weights=True)\n",
    "        depth_edge_weight = torch.load(config['depth_edge_model_ckpt'],\n",
    "                                       map_location=torch.device(device))\n",
    "        depth_edge_model.load_state_dict(depth_edge_weight)\n",
    "        depth_edge_model = depth_edge_model.to(device)\n",
    "        depth_edge_model.eval()\n",
    "\n",
    "        print(f\"Loading depth model at {time.time()}\")\n",
    "        depth_feat_model = Inpaint_Depth_Net()\n",
    "        depth_feat_weight = torch.load(config['depth_feat_model_ckpt'],\n",
    "                                       map_location=torch.device(device))\n",
    "        depth_feat_model.load_state_dict(depth_feat_weight, strict=True)\n",
    "        depth_feat_model = depth_feat_model.to(device)\n",
    "        depth_feat_model.eval()\n",
    "        depth_feat_model = depth_feat_model.to(device)\n",
    "        print(f\"Loading rgb model at {time.time()}\")\n",
    "        rgb_model = Inpaint_Color_Net()\n",
    "        rgb_feat_weight = torch.load(config['rgb_feat_model_ckpt'],\n",
    "                                     map_location=torch.device(device))\n",
    "        rgb_model.load_state_dict(rgb_feat_weight)\n",
    "        rgb_model.eval()\n",
    "        rgb_model = rgb_model.to(device)\n",
    "        graph = None\n",
    "\n",
    "\n",
    "        print(f\"Writing depth ply (and basically doing everything) at {time.time()}\")\n",
    "        rt_info = write_ply(image,\n",
    "                              depth,\n",
    "                              sample['int_mtx'],\n",
    "                              mesh_fi,\n",
    "                              config,\n",
    "                              rgb_model,\n",
    "                              depth_edge_model,\n",
    "                              depth_edge_model,\n",
    "                              depth_feat_model)\n",
    "\n",
    "        if rt_info is False:\n",
    "            continue\n",
    "        rgb_model = None\n",
    "        color_feat_model = None\n",
    "        depth_edge_model = None\n",
    "        depth_feat_model = None\n",
    "        torch.cuda.empty_cache()\n",
    "    if config['save_ply'] is True or config['load_ply'] is True:\n",
    "        verts, colors, faces, Height, Width, hFov, vFov = read_ply(mesh_fi)\n",
    "    else:\n",
    "        verts, colors, faces, Height, Width, hFov, vFov = rt_info\n",
    "\n",
    "\n",
    "    print(f\"Making video at {time.time()}\")\n",
    "    videos_poses, video_basename = copy.deepcopy(sample['tgts_poses']), sample['tgt_name']\n",
    "    top = (config.get('original_h') // 2 - sample['int_mtx'][1, 2] * config['output_h'])\n",
    "    left = (config.get('original_w') // 2 - sample['int_mtx'][0, 2] * config['output_w'])\n",
    "    down, right = top + config['output_h'], left + config['output_w']\n",
    "    border = [int(xx) for xx in [top, down, left, right]]\n",
    "    normal_canvas, all_canvas = output_3d_photo(verts.copy(), colors.copy(), faces.copy(), copy.deepcopy(Height), copy.deepcopy(Width), copy.deepcopy(hFov), copy.deepcopy(vFov),\n",
    "                        copy.deepcopy(sample['tgt_pose']), sample['video_postfix'], copy.deepcopy(sample['ref_pose']), copy.deepcopy(config['video_folder']),\n",
    "                        image.copy(), copy.deepcopy(sample['int_mtx']), config, image,\n",
    "                        videos_poses, video_basename, config.get('original_h'), config.get('original_w'), border=border, depth=depth, normal_canvas=normal_canvas, all_canvas=all_canvas,\n",
    "                        mean_loc_depth=mean_loc_depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def say_hello(to):\n",
    "    \"Say hello to somebody\"\n",
    "    return f'Hello {to}!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Jeffrey!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "say_hello(\"Jeffrey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert say_hello(\"Jeffrey\") == \"Hello Jeffrey!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documenting ec2/docker startup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install nbdev\n",
    "\n",
    "nbdev_install_git_hooks\n",
    "\n",
    "cd pomerantz\n",
    "\n",
    "nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HelloSayer:\n",
    "    \"Say hello to `to` using `say_hello`\"\n",
    "    def __init__(self, to): self.to = to\n",
    "\n",
    "    def say(self):\n",
    "        \"Do the saying\"\n",
    "        return say_hello(self.to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"HelloSayer.say\" class=\"doc_header\"><code>HelloSayer.say</code><a href=\"__main__.py#L6\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>HelloSayer.say</code>()\n",
       "\n",
       "Do the saying"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(HelloSayer.say)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Jeffrey!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = HelloSayer(\"Jeffrey\")\n",
    "o.say()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert o.say() == \"Hello Jeffrey!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 00_og.ipynb.\n",
      "Converted 01_boostmonodepth_utils.ipynb.\n",
      "Converted 02_bilateral_filtering.ipynb.\n",
      "Converted 03_mesh.ipynb.\n",
      "Converted 04_networks.ipynb.\n",
      "Converted 05_utils.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "# Export modules directly from notebook\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
